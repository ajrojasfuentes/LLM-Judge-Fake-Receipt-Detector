{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "md-title",
   "metadata": {},
   "source": [
    "# LLM-Judge Fake Receipt Detector — Demo Notebook\n",
    "\n",
    "Este notebook ejecuta el pipeline completo desde un entorno Jupyter:\n",
    "\n",
    "1. **Setup** — configurar token de HuggingFace y dependencias\n",
    "2. **Dataset** — cargar etiquetas y explorar los datos disponibles\n",
    "3. **Muestreo** — seleccionar receipts REAL/FAKE para la evaluación\n",
    "4. **Análisis Forense** — señales de imagen sin LLM (ELA, ruido, copy-move)\n",
    "5. **Demo de un recibo** — ejecutar los 3 jueces LLM sobre un solo recibo\n",
    "6. **Pipeline completo** — ejecutar todos los recibos muestreados\n",
    "7. **Evaluación** — métricas: accuracy, precision, recall, F1, confusion matrix\n",
    "\n",
    "> **Requisito:** un `HF_TOKEN` válido con acceso a modelos de HuggingFace Inference API.\n",
    "> Los modelos usados son `Qwen/Qwen2.5-VL-72B-Instruct` e `InternVL3-14B` (serverless inference)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-setup",
   "metadata": {},
   "source": [
    "---\n",
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-path",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Asegurar que el root del proyecto está en el path\n",
    "PROJECT_ROOT = Path('..').resolve()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Cambiar el directorio de trabajo al root para que los paths relativos funcionen\n",
    "os.chdir(PROJECT_ROOT)\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup-token",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opción 1: Cargar desde fichero .env (requiere python-dotenv)\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "\n",
    "# Opción 2: Poner el token directamente (NO subir a git)\n",
    "# os.environ['HF_TOKEN'] = 'hf_xxxxxxxxxxxxx'\n",
    "\n",
    "# Verificar que el token está disponible\n",
    "hf_token = os.environ.get('HF_TOKEN', '')\n",
    "if not hf_token:\n",
    "    print(\"WARNING: HF_TOKEN no encontrado. Los jueces LLM fallarán.\")\n",
    "    print(\"Configúralo con: os.environ['HF_TOKEN'] = 'hf_xxx'\")\n",
    "else:\n",
    "    print(f\"HF_TOKEN encontrado: {hf_token[:8]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-dataset",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Cargar Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-load",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.dataset import DatasetManager\n",
    "\n",
    "dm = DatasetManager()\n",
    "\n",
    "# Cargar etiquetas del split de entrenamiento\n",
    "# Usa los CSVs precalculados en data/dataset/findit2/ si existen,\n",
    "# o el fichero raw del dataset extraído como fallback.\n",
    "labels_train = dm.load_labels('train')\n",
    "print(f\"Train — total: {len(labels_train)}, \"\n",
    "      f\"REAL: {sum(v=='REAL' for v in labels_train.values())}, \"\n",
    "      f\"FAKE: {sum(v=='FAKE' for v in labels_train.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-all-splits",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los tres splits\n",
    "all_splits = dm.load_all_splits()\n",
    "\n",
    "rows = []\n",
    "for split, labels in all_splits.items():\n",
    "    real = sum(v == 'REAL' for v in labels.values())\n",
    "    fake = sum(v == 'FAKE' for v in labels.values())\n",
    "    rows.append({'split': split, 'total': len(labels), 'REAL': real, 'FAKE': fake,\n",
    "                 'fake_pct': round(100 * fake / len(labels), 1)})\n",
    "\n",
    "pd.DataFrame(rows).set_index('split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-find-image",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar la imagen de un recibo concreto\n",
    "# (Necesita que el dataset esté descargado y extraído)\n",
    "sample_id = list(labels_train.keys())[0]\n",
    "sample_label = labels_train[sample_id]\n",
    "\n",
    "img_path = dm.find_image(sample_id, 'train')\n",
    "ocr_path = dm.find_ocr_txt(sample_id, 'train')\n",
    "\n",
    "print(f\"ID      : {sample_id}\")\n",
    "print(f\"Label   : {sample_label}\")\n",
    "print(f\"Image   : {img_path}\")\n",
    "print(f\"OCR txt : {ocr_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-download",
   "metadata": {},
   "source": [
    "### 1a. Descargar y extraer el dataset (solo si aún no está extraído)\n",
    "\n",
    "Si `data/raw/findit2/` ya existe, este paso se salta automáticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dataset-download",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DESCOMENTAR para descargar (~400 MB)\n",
    "# dm.download()\n",
    "# dm.extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-sample",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Muestreo de Recibos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampler-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.sampler import ReceiptSampler\n",
    "\n",
    "sampler = ReceiptSampler()\n",
    "print(f\"Config: {sampler.real_count} REAL + {sampler.fake_count} FAKE, \"\n",
    "      f\"split='{sampler.split}', seed={sampler.random_seed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampler-select",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar muestra (reproducible con la semilla fija)\n",
    "labels = dm.load_labels(sampler.split)\n",
    "sample = sampler.sample(labels, dataset_manager=dm)\n",
    "\n",
    "# Mostrar como DataFrame\n",
    "sample_df = pd.DataFrame([\n",
    "    {\n",
    "        'id': r['id'],\n",
    "        'label': r['label'],\n",
    "        'image_found': bool(r.get('image_path')),\n",
    "        'ocr_found': bool(r.get('ocr_txt_path')),\n",
    "    }\n",
    "    for r in sample\n",
    "])\n",
    "print(f\"Muestra: {len(sample_df)} recibos\")\n",
    "print(sample_df['label'].value_counts().to_string())\n",
    "sample_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sampler-save",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar la muestra en outputs/samples.json\n",
    "sampler.save(sample)\n",
    "print(\"Muestra guardada.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-forensic",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Análisis Forense (sin LLM)\n",
    "\n",
    "El `ForensicPipeline` extrae señales de imagen:\n",
    "- **ELA** (Error Level Analysis): detecta artefactos de recompresión JPEG\n",
    "- **Noise map**: bloques con varianza anómala (posibles regiones pegadas)\n",
    "- **Copy-move**: detección de regiones duplicadas dentro del mismo recibo\n",
    "- **OCR**: extrae texto estructurado para verificación aritmética\n",
    "\n",
    "No requiere GPU ni token de HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forensic-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline.forensic_pipeline import ForensicPipeline\n",
    "\n",
    "fp = ForensicPipeline(\n",
    "    output_dir='outputs/forensic',\n",
    "    save_images=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Usar el primer recibo de la muestra que tenga imagen\n",
    "receipt = next(r for r in sample if r.get('image_path'))\n",
    "img_p = Path(receipt['image_path'])\n",
    "ocr_p = receipt.get('ocr_txt_path')\n",
    "\n",
    "print(f\"Analizando: {receipt['id']} ({receipt['label']})\")\n",
    "forensic_ctx = fp.analyze(img_p, ocr_txt_path=ocr_p)\n",
    "\n",
    "print(f\"\\nELA mean error       : {forensic_ctx.ela_mean_error}\")\n",
    "print(f\"ELA suspicious ratio : {forensic_ctx.ela_suspicious_ratio:.1%}\" \n",
    "      if forensic_ctx.ela_suspicious_ratio is not None else \"ELA: N/A\")\n",
    "print(f\"Copy-move confidence : {forensic_ctx.cm_confidence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forensic-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ver el bloque de texto que se añade al prompt del juez\n",
    "print(forensic_ctx.to_prompt_section())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forensic-images",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizar las imágenes forenses guardadas\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "forensic_dir = Path('outputs/forensic')\n",
    "forensic_imgs = sorted(forensic_dir.glob('*.png'))\n",
    "\n",
    "# Mostrar la imagen original + imágenes forenses\n",
    "images_to_show = [img_p] + forensic_imgs[:3]\n",
    "titles = ['Original'] + [p.stem.split('_', 1)[-1] for p in forensic_imgs[:3]]\n",
    "\n",
    "if images_to_show:\n",
    "    fig, axes = plt.subplots(1, len(images_to_show), figsize=(5 * len(images_to_show), 6))\n",
    "    if len(images_to_show) == 1:\n",
    "        axes = [axes]\n",
    "    for ax, path, title in zip(axes, images_to_show, titles):\n",
    "        try:\n",
    "            img = Image.open(path)\n",
    "            ax.imshow(img, cmap='gray' if img.mode == 'L' else None)\n",
    "        except Exception:\n",
    "            ax.imshow(mpimg.imread(str(path)))\n",
    "        ax.set_title(title, fontsize=10)\n",
    "        ax.axis('off')\n",
    "    plt.suptitle(f\"{receipt['id']} ({receipt['label']})\", fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No hay imágenes forenses disponibles. ¿Está el dataset extraído?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-demo",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Demo: Un Recibo con los 3 Jueces LLM\n",
    "\n",
    "Requiere `HF_TOKEN` configurado en el entorno.\n",
    "\n",
    "Los 3 jueces son:\n",
    "| Judge | Modelo | Persona | Temperatura |\n",
    "|-------|--------|---------|-------------|\n",
    "| judge_1 | Qwen2.5-VL-72B | Forensic Accountant | 0.1 |\n",
    "| judge_2 | Qwen2.5-VL-72B | Document Examiner | 0.7 |\n",
    "| judge_3 | InternVL3-14B | Visual Inspector | 0.3 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-single-id",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Escoger un recibo de la muestra (cambia el índice para probar distintos)\n",
    "RECEIPT_IDX = 0\n",
    "\n",
    "receipt = sample[RECEIPT_IDX]\n",
    "receipt_id = receipt['id']\n",
    "ground_truth = receipt['label']\n",
    "\n",
    "img_path = Path(receipt['image_path']) if receipt.get('image_path') else dm.find_image(receipt_id)\n",
    "ocr_path = receipt.get('ocr_txt_path') or dm.find_ocr_txt(receipt_id)\n",
    "\n",
    "print(f\"Receipt ID    : {receipt_id}\")\n",
    "print(f\"Ground Truth  : {ground_truth}\")\n",
    "print(f\"Image         : {img_path}\")\n",
    "print(f\"OCR           : {ocr_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-forensic-ctx",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis forense previo (opcional pero recomendado)\n",
    "USE_FORENSIC = True  # Cambiar a False para omitir\n",
    "\n",
    "forensic_context = None\n",
    "if USE_FORENSIC and img_path and img_path.exists():\n",
    "    fp = ForensicPipeline(output_dir='outputs/forensic', save_images=False, verbose=False)\n",
    "    forensic_context = fp.analyze(img_path, ocr_txt_path=ocr_path)\n",
    "    ela = (f\"{forensic_context.ela_suspicious_ratio:.1%}\" \n",
    "           if forensic_context.ela_suspicious_ratio is not None else 'N/A')\n",
    "    cm = (f\"{forensic_context.cm_confidence:.2f}\" \n",
    "          if forensic_context.cm_confidence is not None else 'N/A')\n",
    "    print(f\"Forensic: ELA={ela}  CopyMove={cm}\")\n",
    "else:\n",
    "    print(\"Forensic skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-run-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "from judges.qwen_judge import make_forensic_accountant, make_document_examiner\n",
    "from judges.internvl_judge import InternVLJudge\n",
    "from judges.voting import VotingEngine\n",
    "\n",
    "# Instanciar los 3 jueces y el motor de votación\n",
    "judges = [\n",
    "    make_forensic_accountant(),\n",
    "    make_document_examiner(),\n",
    "    InternVLJudge(),\n",
    "]\n",
    "engine = VotingEngine()\n",
    "\n",
    "# Ejecutar cada juez\n",
    "judge_results = []\n",
    "for judge in judges:\n",
    "    print(f\"\\nEjecutando {judge.judge_name}...\", end=' ', flush=True)\n",
    "    result = judge.judge(\n",
    "        receipt_id=receipt_id,\n",
    "        image_path=img_path,\n",
    "        forensic_context=forensic_context,\n",
    "    )\n",
    "    print(f\"{result.label} ({result.confidence:.1f}%)\")\n",
    "    judge_results.append(result)\n",
    "    print(json.dumps(result.to_dict(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demo-verdict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veredicto final por votación\n",
    "verdict = engine.aggregate(judge_results)\n",
    "\n",
    "is_correct = verdict.label == ground_truth\n",
    "print(\"=\" * 50)\n",
    "print(f\"VEREDICTO FINAL : {verdict.label}\")\n",
    "print(f\"Ground Truth    : {ground_truth}\")\n",
    "print(f\"Resultado       : {'✓ CORRECTO' if is_correct else '✗ ERROR'}\")\n",
    "print(f\"Tally           : {verdict.tally}\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(verdict.to_dict(), indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-full-pipeline",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Pipeline Completo — Todos los Recibos Muestreados\n",
    "\n",
    "Ejecuta los 3 jueces sobre los 20 recibos de la muestra y guarda los resultados en `outputs/results/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-pipeline-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Cargar config del voting engine\n",
    "with open('configs/judges.yaml') as f:\n",
    "    cfg = yaml.safe_load(f)\n",
    "voting_cfg = cfg.get('voting', {})\n",
    "\n",
    "judges = [\n",
    "    make_forensic_accountant(),\n",
    "    make_document_examiner(),\n",
    "    InternVLJudge(),\n",
    "]\n",
    "engine = VotingEngine(\n",
    "    strategy=voting_cfg.get('strategy', 'majority'),\n",
    "    uncertain_threshold=voting_cfg.get('uncertain_threshold', 2),\n",
    ")\n",
    "\n",
    "# Cargar muestra guardada\n",
    "sample_loaded = sampler.load()\n",
    "print(f\"Muestra cargada: {len(sample_loaded)} recibos\")\n",
    "\n",
    "# Carpeta de resultados\n",
    "results_dir = Path('outputs/results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "full-pipeline-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar pipeline completo\n",
    "USE_FORENSIC = True\n",
    "\n",
    "fp = ForensicPipeline(output_dir='outputs/forensic', save_images=False, verbose=False) if USE_FORENSIC else None\n",
    "\n",
    "run_results = []\n",
    "for receipt in sample_loaded:\n",
    "    rid = receipt['id']\n",
    "    img_p = Path(receipt['image_path']) if receipt.get('image_path') else dm.find_image(rid)\n",
    "    ocr_p = receipt.get('ocr_txt_path') or dm.find_ocr_txt(rid)\n",
    "\n",
    "    if img_p is None or not img_p.exists():\n",
    "        print(f\"[SKIP] {rid} — imagen no encontrada\")\n",
    "        continue\n",
    "\n",
    "    # Análisis forense\n",
    "    fctx = None\n",
    "    if fp:\n",
    "        fctx = fp.analyze(img_p, ocr_txt_path=ocr_p)\n",
    "        ela = f\"{fctx.ela_suspicious_ratio:.0%}\" if fctx.ela_suspicious_ratio is not None else 'N/A'\n",
    "    else:\n",
    "        ela = 'OFF'\n",
    "\n",
    "    # Jueces\n",
    "    jresults = []\n",
    "    for judge in judges:\n",
    "        r = judge.judge(receipt_id=rid, image_path=img_p, forensic_context=fctx)\n",
    "        jresults.append(r)\n",
    "\n",
    "    verdict = engine.aggregate(jresults)\n",
    "    output = verdict.to_dict()\n",
    "    output['ground_truth'] = receipt['label']\n",
    "    output['forensic_used'] = USE_FORENSIC\n",
    "\n",
    "    # Guardar JSON de resultado\n",
    "    out_path = results_dir / f\"{rid}.json\"\n",
    "    with open(out_path, 'w') as f:\n",
    "        json.dump(output, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    match = 'OK' if verdict.label == receipt['label'] else 'WRONG'\n",
    "    print(f\"{rid[:30]:30s} GT={receipt['label']:4s} → {verdict.label:4s} ELA={ela:>5s} [{match}]\")\n",
    "    run_results.append(output)\n",
    "\n",
    "print(f\"\\nProcesados: {len(run_results)}/{len(sample_loaded)} recibos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-eval",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Evaluación de Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pipeline.evaluator import Evaluator\n",
    "\n",
    "ground_truth_map = {r['id']: r['label'] for r in sample_loaded}\n",
    "\n",
    "ev = Evaluator()\n",
    "ev.load_results()\n",
    "\n",
    "summary = ev.summary(ground_truth_map)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = summary['confusion_matrix']\n",
    "cm_matrix = [\n",
    "    [cm.get('TP', 0), cm.get('FN', 0)],\n",
    "    [cm.get('FP', 0), cm.get('TN', 0)],\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 4))\n",
    "sns.heatmap(\n",
    "    cm_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Pred FAKE', 'Pred REAL'],\n",
    "    yticklabels=['GT FAKE', 'GT REAL'],\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/eval_confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-per-judge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy por juez individual\n",
    "judge_stats = {}\n",
    "for result in ev._results:\n",
    "    gt = ground_truth_map.get(result['receipt_id'])\n",
    "    if gt is None:\n",
    "        continue\n",
    "    for j in result.get('judges', []):\n",
    "        name = j['judge_name']\n",
    "        if name not in judge_stats:\n",
    "            judge_stats[name] = {'correct': 0, 'total': 0}\n",
    "        judge_stats[name]['total'] += 1\n",
    "        if j['label'] == gt:\n",
    "            judge_stats[name]['correct'] += 1\n",
    "\n",
    "print(\"=== ACCURACY POR JUEZ ===\")\n",
    "for name, stats in judge_stats.items():\n",
    "    acc = stats['correct'] / max(stats['total'], 1)\n",
    "    print(f\"  {name:25s}: {acc:.1%}  ({stats['correct']}/{stats['total']})\")\n",
    "\n",
    "print(f\"\\n=== VOTING FINAL ===\")\n",
    "print(f\"  Accuracy : {summary.get('accuracy', 0):.1%}\")\n",
    "print(f\"  Precision: {summary.get('precision', 0):.1%}\")\n",
    "print(f\"  Recall   : {summary.get('recall', 0):.1%}\")\n",
    "print(f\"  F1 Score : {summary.get('f1', 0):.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eval-disagreements",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Casos donde los jueces no coinciden\n",
    "cases = ev.disagreement_cases(n=5)\n",
    "if cases:\n",
    "    print(\"=== CASOS DE DESACUERDO ENTRE JUECES ===\")\n",
    "    for case in cases:\n",
    "        gt = ground_truth_map.get(case['receipt_id'], '?')\n",
    "        print(f\"\\n--- {case['receipt_id']} ---\")\n",
    "        print(f\"  Ground truth : {gt}\")\n",
    "        print(f\"  Veredicto    : {case.get('label', '?')} | {case.get('tally', '')}\")\n",
    "        for j in case.get('judges', []):\n",
    "            print(f\"  [{j['judge_name']:20s}] {j['label']:4s} ({j['confidence']:.0f}%)\")\n",
    "            for r in j.get('reasons', [])[:2]:\n",
    "                print(f\"    • {r}\")\n",
    "else:\n",
    "    print(\"No hay casos de desacuerdo o aún no se han ejecutado los jueces.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "md-cli",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Equivalente CLI\n",
    "\n",
    "Todos los pasos anteriores también se pueden ejecutar desde la línea de comandos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cli-equiv",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equivalentes CLI — solo visualización, no ejecutan\n",
    "cli_commands = [\n",
    "    (\"Descargar dataset\",          \"python main.py download\"),\n",
    "    (\"Muestrear 20 recibos\",       \"python main.py sample\"),\n",
    "    (\"Ejecutar pipeline\",           \"python main.py run\"),\n",
    "    (\"Pipeline + forense\",          \"python main.py run --forensic\"),\n",
    "    (\"Evaluar resultados\",          \"python main.py evaluate\"),\n",
    "    (\"Demo un recibo\",              \"python main.py demo X00016469622\"),\n",
    "    (\"Demo recibo + forense\",       \"python main.py demo X00016469622 --forensic\"),\n",
    "    (\"Solo análisis forense\",       \"python main.py forensic X00016469622\"),\n",
    "]\n",
    "\n",
    "print(\"=== COMANDOS CLI EQUIVALENTES ===\")\n",
    "for desc, cmd in cli_commands:\n",
    "    print(f\"  {desc:30s}  →  {cmd}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cli-run",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ejecutar un comando CLI desde el notebook:\n",
    "# !python ../main.py sample\n",
    "# !python ../main.py run --forensic\n",
    "# !python ../main.py evaluate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
