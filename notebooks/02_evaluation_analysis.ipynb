{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Analysis — LLM Judge Results\n",
    "\n",
    "This notebook loads all judge results and analyses:\n",
    "- Overall accuracy, precision, recall, F1\n",
    "- Confusion matrix\n",
    "- Disagreement cases between judges\n",
    "- Per-flag analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from pipeline.evaluator import Evaluator\n",
    "from pipeline.sampler import ReceiptSampler\n",
    "\n",
    "sns.set_theme(style='whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ReceiptSampler()\n",
    "sample = sampler.load()\n",
    "ground_truth = {r['id']: r['label'] for r in sample}\n",
    "\n",
    "ev = Evaluator()\n",
    "ev.load_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = ev.summary(ground_truth)\n",
    "print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Confusion Matrix Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = summary['confusion_matrix']\n",
    "cm_matrix = [\n",
    "    [cm.get('TP', 0), cm.get('FN', 0)],\n",
    "    [cm.get('FP', 0), cm.get('TN', 0)],\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(\n",
    "    cm_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "    xticklabels=['Pred REAL', 'Pred FAKE'],\n",
    "    yticklabels=['GT FAKE', 'GT REAL'],\n",
    "    ax=ax\n",
    ")\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/eval_confusion_matrix.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Disagreement Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cases = ev.disagreement_cases(n=5)\n",
    "for case in cases:\n",
    "    gt = ground_truth.get(case['receipt_id'], '?')\n",
    "    print(f\"\\n--- Receipt: {case['receipt_id']} ---\")\n",
    "    print(f\"  Ground truth: {gt}\")\n",
    "    print(f\"  Final verdict: {case['tally']}\")\n",
    "    for j in case.get('judges', []):\n",
    "        print(f\"  [{j['judge_name']:20s}] {j['label']:10s} ({j['confidence']:.0f}%)\")\n",
    "        for r in j.get('reasons', [])[:2]:\n",
    "            print(f\"    • {r}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Flag Frequency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_flags = Counter()\n",
    "for r in ev._results:\n",
    "    all_flags.update(r.get('all_flags', []))\n",
    "\n",
    "if all_flags:\n",
    "    flag_df = pd.DataFrame(all_flags.most_common(), columns=['Flag', 'Count'])\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    sns.barplot(data=flag_df, x='Flag', y='Count', ax=ax)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.set_title('Flag Frequency Across All Receipts')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/eval_flag_frequency.png', dpi=150)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No flags found in results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Per-Judge Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge_stats = {}\n",
    "for result in ev._results:\n",
    "    gt = ground_truth.get(result['receipt_id'], None)\n",
    "    if gt is None:\n",
    "        continue\n",
    "    for j in result.get('judges', []):\n",
    "        name = j['judge_name']\n",
    "        if name not in judge_stats:\n",
    "            judge_stats[name] = {'correct': 0, 'total': 0}\n",
    "        judge_stats[name]['total'] += 1\n",
    "        if j['label'] == gt:\n",
    "            judge_stats[name]['correct'] += 1\n",
    "\n",
    "for name, stats in judge_stats.items():\n",
    "    acc = stats['correct'] / max(stats['total'], 1)\n",
    "    print(f\"  {name:25s}  accuracy: {acc:.1%}  ({stats['correct']}/{stats['total']})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
